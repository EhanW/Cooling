Killed
Killed
Traceback (most recent call last):
  File "/data/yhwang/projects/Cooling/at_cooling.py", line 187, in <module>
    pgd_at_cooling()
  File "/data/yhwang/projects/Cooling/at_cooling.py", line 118, in pgd_at_cooling
    rewarming_list[epoch+args.cooling_interval] = cooling_avg_loss
IndexError: list assignment index out of range
Traceback (most recent call last):
  File "/data/yhwang/projects/Cooling/at_cooling.py", line 187, in <module>
    pgd_at_cooling()
  File "/data/yhwang/projects/Cooling/at_cooling.py", line 118, in pgd_at_cooling
    rewarming_list[epoch+args.cooling_interval] = cooling_avg_loss
IndexError: list assignment index out of range
Traceback (most recent call last):
  File "/data/yhwang/projects/Cooling/at_cooling.py", line 187, in <module>
    pgd_at_cooling()
  File "/data/yhwang/projects/Cooling/at_cooling.py", line 118, in pgd_at_cooling
    rewarming_list[epoch+args.cooling_interval] = cooling_avg_loss
IndexError: list assignment index out of range

Epoch0
Epoch1
Epoch2
Epoch3
Epoch4
Epoch5
Epoch6
Epoch7
Epoch8
Epoch9
Epoch10
Epoch11
Epoch12
Epoch13
Epoch14
Epoch15
Epoch0
Epoch1
Epoch2
Epoch3
Epoch4
Epoch5
Epoch6
Epoch7
Epoch8
Epoch9
Epoch10
Epoch11
Traceback (most recent call last):
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/queues.py", line 244, in _feed
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 51, in dumps
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 359, in reduce_storage
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 198, in DupFd
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/resource_sharer.py", line 48, in __init__
OSError: [Errno 24] Too many open files
Traceback (most recent call last):
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/resource_sharer.py", line 145, in _serve
Process Process-11:
    send(conn, destination_pid)
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/resource_sharer.py", line 50, in send
    reduction.send_handle(conn, new_fd, pid)
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 183, in send_handle
    with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s:
  File "/data/yhwang/anaconda3/lib/python3.9/socket.py", line 544, in fromfd
    nfd = dup(fd)
OSError: [Errno 24] Too many open files
Traceback (most recent call last):
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 269, in _worker_loop
    r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 297, in rebuild_storage_fd
    fd = df.detach()
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/resource_sharer.py", line 58, in detach
    return reduction.recv_handle(conn)
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 189, in recv_handle
    return recvfds(s, 1)[0]
  File "/data/yhwang/anaconda3/lib/python3.9/multiprocessing/reduction.py", line 159, in recvfds
    raise EOFError
EOFError
Traceback (most recent call last):
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1163, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/data/yhwang/anaconda3/lib/python3.9/queue.py", line 180, in get
    self.not_empty.wait(remaining)
  File "/data/yhwang/anaconda3/lib/python3.9/threading.py", line 316, in wait
    gotit = waiter.acquire(True, timeout)
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3894033) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/yhwang/projects/Cooling/shapley.py", line 181, in <module>
    dgs.run()
  File "/data/yhwang/projects/Cooling/shapley.py", line 71, in run
    marginals, adv_marginals = self.one_permutation()
  File "/data/yhwang/projects/Cooling/shapley.py", line 95, in one_permutation
    self.retrain(retrain_loader)
  File "/data/yhwang/projects/Cooling/shapley.py", line 140, in retrain
    self.retrain_epoch(loader)
  File "/data/yhwang/projects/Cooling/shapley.py", line 143, in retrain_epoch
    for id, (data, target, index) in enumerate(loader):
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 681, in __next__
    data = self._next_data()
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1359, in _next_data
    idx, data = self._get_data()
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1315, in _get_data
    success, data = self._try_get_data()
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1176, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 3894033) exited unexpectedly
torch.Size([2987, 1])
torch.Size([16394, 1])
torch.Size([9732, 1])
torch.Size([6582, 1])
torch.Size([14305, 1])
Traceback (most recent call last):
  File "/data/yhwang/projects/Cooling/shapley.py", line 212, in <module>
    dgs.run()
  File "/data/yhwang/projects/Cooling/shapley.py", line 89, in run
    marginals, adv_marginals = self.one_permutation()
  File "/data/yhwang/projects/Cooling/shapley.py", line 115, in one_permutation
    self.retrain(indices)
  File "/data/yhwang/projects/Cooling/shapley.py", line 162, in retrain
    self.retrain_epoch(indices)
  File "/data/yhwang/projects/Cooling/shapley.py", line 174, in retrain_epoch
    data = pgd_inf(self.model, data, target, args.epsilon, args.alpha, args.steps, args.random_start)
  File "/data/yhwang/projects/Cooling/utils/pgd.py", line 13, in pgd_inf
    loss = F.cross_entropy(model(delta+data), target)
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/yhwang/projects/Cooling/networks/resnet.py", line 83, in forward
    out = F.relu(self.bn1(self.conv1(x)))
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 457, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/data/yhwang/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 453, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 1, 3, 32, 32]
